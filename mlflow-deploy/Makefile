# --- Variables ---
# Define el puerto de servicio
PORT = 5001

# Define el nombre del modelo (simula el uso de MLflow Model Registry)
MODEL_NAME = best_model

# --- Targets de Utilidad ---

install:
	@echo "âš™ï¸ Instalando dependencias..."
	pip install -r requirements.txt

clean:
	@echo "ğŸ—‘ï¸ Limpiando artefactos de MLflow..."
	rm -rf mlruns/
	
# --- Targets de Entrenamiento y ValidaciÃ³n ---

train-linear:
	@echo "ğŸš€ Ejecutando entrenamiento del modelo de RegresiÃ³n Lineal..."
	python src/train_linear.py

train-logistic:
	@echo "ğŸš€ Ejecutando entrenamiento del modelo de RegresiÃ³n LogÃ­stica..."
	python src/train_logistic.py

validate:
	@echo "âœ… Ejecutando validaciÃ³n del Ãºltimo Run de MLflow..."
	python src/validate.py

run-all: train-linear validate train-logistic validate

# --- Targets de Servicio/Despliegue ---

# 1. OpciÃ³n simple (Servicio local del Ãºltimo run - Requiere saber la ruta)
# Esto es difÃ­cil de automatizar sin scripting, pero es el comando directo.
# Si quieres correr un modelo manualmente: make serve_manual path=/path/to/run/artifacts/model
serve_manual:
	@echo "ğŸŒ Sirviendo el modelo desde la ruta especificada..."
	mlflow models serve -m $(path) -p $(PORT) --no-conda

# 2. OpciÃ³n Avanzada (Simulando Model Registry)
# En un ambiente real de MLOps, el 'validate.py' promoverÃ­a el mejor modelo
# a un nombre especÃ­fico en el Model Registry, y servirÃ­as por nombre.
# NOTA: Para que esto funcione, necesitarÃ¡s un servidor MLflow remoto o 
# configurar el Model Registry en local.
serve_registry:
	@echo "ğŸŒ Sirviendo la Ãºltima versiÃ³n del modelo registrado 'best_model'..."
	mlflow models serve -m models:/"$(MODEL_NAME)"/latest -p $(PORT) --no-conda